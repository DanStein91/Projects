{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Data Science in Python\n",
    "## Daniel Stein 19201398"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import nltk\n",
    "from requests import get\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bs4 import Comment\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tokenize = CountVectorizer().build_tokenizer()\n",
    "from sklearn.feature_extraction import text\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1:Scrape reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step I chose to get Auto, Gym, and Hotel reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we will store the three categories we want to extract in a list. \n",
    "# They are written as it is in the URL for any specific category.\n",
    "# I also created empty lists where I will be storing all the links within the categories (1 for each category). \n",
    "\n",
    "rawhtml = [\"automotive_list\",\"gym_list\",\"hotels_list\"]\n",
    "links_auto = []\n",
    "links_gym = []\n",
    "links_hotel = []\n",
    "\n",
    "# I loop through the list of categories and add to the url where the review links are\n",
    "for i in rawhtml:\n",
    "#get content from url (elements)\n",
    "    html_page = requests.get(\"http://mlg.ucd.ie/modules/yalp/\"+i+\".html\")\n",
    "    #nested structure for easy reading\n",
    "    soup = BS(html_page.text, 'html.parser')\n",
    "    \n",
    "    #If statement to check and store for each category\n",
    "    if i == \"automotive_list\":\n",
    "        #go through part of element where we find links\n",
    "        for link in soup.findAll('a'):\n",
    "            #get link\n",
    "            links_auto.append(link.get('href'))\n",
    "        #remove first element as it is irrelevant\n",
    "        links_auto.remove(links_auto[0])\n",
    "# Repeat for the other 2 categories\n",
    "    if i == \"gym_list\":\n",
    "        for link in soup.findAll('a'):\n",
    "                links_gym.append(link.get('href'))\n",
    "        links_gym.remove(links_gym[0])\n",
    "        \n",
    "    if i == \"hotels_list\":\n",
    "        for link in soup.findAll('a'):\n",
    "                links_hotel.append(link.get('href'))\n",
    "        links_hotel.remove(links_hotel[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the lists where the scores and comments will be stored\n",
    "auto_score = []\n",
    "auto_text = []\n",
    "gym_score = []\n",
    "gym_text = []\n",
    "hotel_score = []\n",
    "hotel_text = []\n",
    "\n",
    "#loop through categories\n",
    "for x in rawhtml:\n",
    "    #if statement to separate categories\n",
    "    if x == \"automotive_list\":\n",
    "        #for each linked saved from the category url\n",
    "        for y in links_auto:\n",
    "            html_page = requests.get(\"http://mlg.ucd.ie/modules/yalp/\"+y)\n",
    "            soup = BS(html_page.text, 'html.parser')\n",
    "            #extract from elements the rating part\n",
    "            for link1 in soup.findAll('p', class_=\"rating\"):\n",
    "                link1 = link1.find('img')\n",
    "                #give it a value of negative or positive depending on the stars\n",
    "                if link1.get('alt') in (\"1-star\",\"2-star\", \"3-star\"): \n",
    "                    link1 = \"negative\"\n",
    "                else: \n",
    "                    link1 = \"positive\"\n",
    "                #append to score list for the specific category\n",
    "                auto_score.append(link1)\n",
    "            #get text revies from each link\n",
    "            for link2 in soup.findAll('p', class_=\"review-text\"):\n",
    "                #make into plain text.\n",
    "                link2 = link2.text.strip()\n",
    "                auto_text.append(link2)\n",
    "        #Store in a dataframe both score and text\n",
    "        df_auto = pd.DataFrame(list(zip(auto_score, auto_text)), columns = ['Score','Text'])    \n",
    "#repeat for other 2 categories  \n",
    "    if x == \"gym_list\":        \n",
    "        for y in links_gym:\n",
    "            html_page = requests.get(\"http://mlg.ucd.ie/modules/yalp/\"+y)\n",
    "            soup = BS(html_page.text, 'html.parser')\n",
    "            for link1 in soup.findAll('p', class_=\"rating\"):\n",
    "                link1 = link1.find('img')\n",
    "                if link1.get('alt') in (\"1-star\",\"2-star\", \"3-star\"): \n",
    "                    link1 = \"negative\"\n",
    "                else: \n",
    "                    link1 = \"positive\"\n",
    "                gym_score.append(link1)\n",
    "            for link2 in soup.findAll('p', class_=\"review-text\"):\n",
    "                link2 = link2.text.strip()\n",
    "                gym_text.append(link2)\n",
    "        df_gym = pd.DataFrame(list(zip(gym_score, gym_text)), columns = ['Score','Text'])\n",
    "        \n",
    "    if x == \"hotels_list\":    \n",
    "        for y in links_hotel:\n",
    "            html_page = requests.get(\"http://mlg.ucd.ie/modules/yalp/\"+y)\n",
    "            soup = BS(html_page.text, 'html.parser')\n",
    "            for link1 in soup.findAll('p', class_=\"rating\"):\n",
    "                link1 = link1.find('img')\n",
    "                if link1.get('alt') in (\"1-star\",\"2-star\", \"3-star\"): \n",
    "                    link1 = \"negative\"\n",
    "                else: \n",
    "                    link1 = \"positive\"\n",
    "                hotel_score.append(link1)\n",
    "            for link2 in soup.findAll('p', class_=\"review-text\"):\n",
    "                link2 = link2.text.strip()\n",
    "                hotel_text.append(link2)\n",
    "        df_hotel = pd.DataFrame(list(zip(hotel_score, hotel_text)), columns = ['Score','Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the first cell of code, we have created a list containing stopwords using sklearn. Below is an extention of words or numbers that are irrelevant when classifying, and that might actually negatively affect the results. I created this list after running the classification and displaying the terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_num =('00', '000', '10', '100', '101', '10am', '11', '110', '11am', '12',\n",
    "                '13', '14', '15', '150', '16', '17', '18', '19', '1am', '1st', '20', \n",
    "                '200', '2009', '2010', '2012', '2013', '2014', '2015', '2016', '2017', \n",
    "                '22', '24', '25', '2am', '2nd', '2pm', '30', '300', '30am', '30pm', \n",
    "                '30th', '35', '39', '3am', '3pm', '3rd', '40', '400', '45', '48','4pm',\n",
    "                '4th', '50', '500', '55', '5pm', '5th', '60', '600', '65', '6am', '70',\n",
    "                '75', '80', '800', '8am', '8th', '90', '95', '99', '9am','3000', '32',\n",
    "                '350', '450', '4runner', '5000', '59', '6pm', '700', '750', '7am', '900', '9am'\n",
    "               '00am', '00pm', '02', '06', '07', '08', '09', '1000', '105', '10pm', '10th', '115',\n",
    "                '11pm', '120', '1200', '125', '12pm', '130', '15th', '160', '16th', '180', '1pm',\n",
    "                '2005', '2007', '2008', '2011', '2018', '202', '21', '215', '23', '250', '26', '260',\n",
    "                '27', '28', '29', '30mins', '33', '34', '35th', '36', '360', '38', '3x', '43', '458',\n",
    "                '45am', '47','49', '52', '5am', '63', '67', '69', '72', '77', '79', '7pm', '7th',\n",
    "                '80s', '85', '89', '8pm', '98', '9pm','00am','140', '1500', '170', '1800', '2000', '2003', '2004', '2006')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the steps for preprocessing all the text reviews. I made them all into lower case and remove all stop words identified by the two lists mentioned before. I then detokenized the text and replaced it in the dataframe according to the row number named \"count\". I repeated this step for all the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#create a count that will increase as loop runs\n",
    "count = 0\n",
    "#go through dataframe text column\n",
    "for i in df_auto['Text']:\n",
    "    #lower case the text\n",
    "    lower = i.lower()\n",
    "    #tokenize text and make variable\n",
    "    token = tokenize(lower)    \n",
    "    stop_rm = []\n",
    "    for tokens in token:\n",
    "        if not tokens in stopwords and not tokens in stopwords_num:\n",
    "            stop_rm.append(tokens)\n",
    "    text = TreebankWordDetokenizer().detokenize(stop_rm)\n",
    "    df_auto.iloc[count,1] = text\n",
    "    count += 1\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#create a count that will increase as loop runs\n",
    "count = 0\n",
    "#go through dataframe text column\n",
    "for i in df_gym['Text']:\n",
    "    #lower case the text\n",
    "    lower = i.lower()\n",
    "    #tokenize text and make variable\n",
    "    token = tokenize(lower)    \n",
    "    stop_rm = []\n",
    "    for tokens in token:\n",
    "        if not tokens in stopwords and not tokens in stopwords_num:\n",
    "            stop_rm.append(tokens)\n",
    "    text = TreebankWordDetokenizer().detokenize(stop_rm)\n",
    "    df_gym.iloc[count,1] = text\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "#create a count that will increase as loop runs\n",
    "count = 0\n",
    "#go through dataframe text column\n",
    "for i in df_hotel['Text']:\n",
    "    #lower case the text\n",
    "    lower = i.lower()\n",
    "    #tokenize text and make variable\n",
    "    token = tokenize(lower)    \n",
    "    stop_rm = []\n",
    "    for tokens in token:\n",
    "        if not tokens in stopwords and not tokens in stopwords_num:\n",
    "            stop_rm.append(tokens)\n",
    "    text = TreebankWordDetokenizer().detokenize(stop_rm)\n",
    "    df_hotel.iloc[count,1] = text\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created one last dataframe with all the data from the three previously processed dataframes. This holds all the information for all categories. I will later show why I did this and how this dataframe was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_auto,df_gym,df_hotel])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 :Classification and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, I will be using the Random Forest Classifier with an n_estimator of 100 (number of trees in the forest). \n",
    "\n",
    "### Auto Reviews\n",
    "\n",
    "Below we can see for the auto dataset, that there are 2000 documents and 2449 distinct terms in those documents. While the number of document is always the same across all datasets, the number of terms tend to differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2449)\n"
     ]
    }
   ],
   "source": [
    "#Here I create variables containing each field of the dataframes that will be separated and used in the training and testing process\n",
    "document_auto = df_auto['Text']\n",
    "target_auto = df_auto['Score']\n",
    "\n",
    "#We creare a vectorizer\n",
    "vectorizer =CountVectorizer(min_df = 5)\n",
    "#We create the table of words it will show the number of documents and distinct terms\n",
    "X_auto = vectorizer.fit_transform(document_auto)\n",
    "print(X_auto.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary has 2449 distinct terms\n"
     ]
    }
   ],
   "source": [
    "#Here we create a list that holds all the distinct terms (note that X holds numerical values and terms holds written words)\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(\"Vocabulary has %d distinct terms\" % len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an extract of the first 50 terms from the Auto dataframe, we can see that numbers were removed (numbers are always first) in the preprocessing steps. We also see there are a few words that tend to be neutral and not holding any sentiment such as names. We could create more lists or even download one so that we could only keep words expressing sentiment. I decided not to apply this step, as names will be rather rare and overall I believe this words will not skew the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'able', 'absolute', 'absolutely', 'ac', 'accent', 'accept', 'accident', 'accommodate', 'accommodating', 'accord', 'according', 'account', 'accurate', 'ace', 'actual', 'actually', 'acura', 'ad', 'adam', 'add', 'added', 'adding', 'addition', 'additional', 'address', 'addressed', 'adjusted', 'adjuster', 'admit', 'admitted', 'advance', 'advantage', 'advertise', 'advertised', 'advice', 'advise', 'advised', 'advisor', 'afford', 'affordable', 'afraid', 'aftermarket', 'afternoon', 'age', 'agent', 'ago', 'agree', 'agreed']\n"
     ]
    }
   ],
   "source": [
    "# Display a set of sample terms, I used this to verify if there was a cluster of words or numbers that I could\n",
    "#then put in a seperate stopword list that was used in the preprocessing step\n",
    "print(terms[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the variables for the training and test data. Below we specify what values we are using and how much of the data we are using for training and testing: I decided to use 70% for training and 30% for testing. I think these ratios are balanced, and I will avoid over-fiiting and underfitting my classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 1400 examples\n",
      "Test set has 600 examples\n"
     ]
    }
   ],
   "source": [
    "#Here we stablish our training and test data and results and divide it into 70/30 for training/test.\n",
    "data_train, data_test, target_train, target_test = train_test_split(X_auto, target_auto, test_size=0.30)\n",
    "print(\"Training set has %d examples\" % data_train.shape[0] )\n",
    "print(\"Test set has %d examples\" % data_test.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our classifier.\n",
    "clf=RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we run the classifier using the data provided above and store the results.\n",
    "clf.fit(data_train,target_train)\n",
    "predict = clf.predict(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the classifier we can store the results, and compare our predicted data to the actual results in order to get an accuracy score. Below we see the accuracy for a single run (this will number normally change as we do more runs), and it retains an accuracy of between 80% to 90% in almost most cases. This shows that our classifier is correctly classifying text into positive or negative with high accuracy (considering we are dealing with sentiment and text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 0.8716666666666667\n"
     ]
    }
   ],
   "source": [
    "#Here we get the accuracy of our classifier by taking the predicted values and comparing with the actual results.\n",
    "print(\"The accuracy is:\", accuracy_score(target_test, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the accuracy of a single run, we can use more data to evaluate our results. Below we see the confusion matrix for our results. In this matrix our horizontal values are predicted results while our vertical are actual results. From this we can see that in the majority ofthe cases, negative reviews were incorrectly claffied more often than positive ones. We can also see that there are overall more positive reviews in our data than negative ones, this reflects on our matrix as most of the time we will get the sum of the lowe cells to be much higher than the sum of the top cells (in this test data most of the time there are also more positive than negative). \n",
    "\n",
    "We can see on the report bellow the different statistics form our classifier:\n",
    "Precision: of the reviews identified as either positive or negative, how many were actually correct. \n",
    "Recall: of all the positive and negative how many were identified correctly.\n",
    "F1-score: This shows the balance between precision and recall.\n",
    "\n",
    "From the statistics we can note a few things:\n",
    "\n",
    "Negative: Overall precision on negative reviews is rather high, this means that of the reviews classified as negative, most of them were actual negatives. As for recall, we see that it is rather low in comparison with the other statistics, this means that of all the actual negative reviews not that many of them were classified as negative, this brings down the balance of f1-score, and overall brings down accuracy for our classifier.\n",
    "\n",
    "Positive: For the positive reviews we see that in general it scores well in precision, the fact that it scored so high in precision even if we know that a lot of negative were predicted as positive is due to how many more positive reviews there are compared to negative. As we can see recall has a very high score, which means of all the positive reviews most of them were classified correctll. The combination of these two give a high f1-score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[184  53]\n",
      " [ 24 339]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.78      0.83       237\n",
      "    positive       0.86      0.93      0.90       363\n",
      "\n",
      "    accuracy                           0.87       600\n",
      "   macro avg       0.87      0.86      0.86       600\n",
      "weighted avg       0.87      0.87      0.87       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a confusion matrix and further evaluation results.\n",
    "cm = confusion_matrix(target_test, predict,labels=['negative','positive'])\n",
    "print(cm)\n",
    "print(\"\\n\")\n",
    "print(classification_report(target_test, predict, target_names=[\"negative\",\"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can do a cross validation evaluation, which will help avoid any over-fitting.\n",
    "\n",
    "Cross validation runs our classifier a specific number of times and divides the training and testing data differently each time. By doing so, come across as many posible scenarios as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90547264, 0.90049751, 0.93      , 0.875     , 0.855     ,\n",
       "       0.81      , 0.825     , 0.855     , 0.87437186, 0.89447236])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross validation using 10 folds which will give 10 accuracies that will be stored in a list.\n",
    "cross_val_auto =  cross_val_score(clf,X_auto, target_auto, cv=10)\n",
    "cross_val_auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cross validation, we can find an average accuracy that is closer to actual accuracy together with standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "#Average and standard deviation from cross validation\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (cross_val_auto.mean(), cross_val_auto.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the same process for all categories and analyze the classifier when used on each one.\n",
    "\n",
    "### Gym Reviews\n",
    "\n",
    "Now we will apply the same steps for the gym reviews dataset.\n",
    "\n",
    "Below, we see the number of documents is again 2000 while the number of terms increase to 2799.\n",
    "\n",
    "We can also see that again there are in average more positive reviews in the test data (confusion matrix) than negative reviews, and more incorrectly identified negative reviews. This follows the same pattern as the previews dataset.\n",
    "\n",
    "We can also observe a similar pattern with the statistics as the previous dataset for both negative and positive, which could be because both datasets contain more positive than negative reviews. Which means positive data has more scenarios covered and so the classifier performs when classifying positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2799)\n",
      "[[141  57]\n",
      " [ 27 375]]\n",
      "The accuracy is: 0.86\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.71      0.77       198\n",
      "    positive       0.87      0.93      0.90       402\n",
      "\n",
      "    accuracy                           0.86       600\n",
      "   macro avg       0.85      0.82      0.83       600\n",
      "weighted avg       0.86      0.86      0.86       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_gym = df_gym['Text']\n",
    "target_gym = df_gym['Score']\n",
    "\n",
    "\n",
    "\n",
    "X_gym= vectorizer.fit_transform(document_gym)\n",
    "print(X_gym.shape)\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(X_gym, target_gym, test_size=0.30)\n",
    "\n",
    "clf.fit(data_train,target_train)\n",
    "predict = clf.predict(data_test)\n",
    "cm = confusion_matrix(target_test, predict,labels=['negative','positive'])\n",
    "print(cm)\n",
    "print(\"The accuracy is:\", accuracy_score(target_test, predict))\n",
    "print(\"\\n\")\n",
    "print(classification_report(target_test, predict, target_names=[\"negative\",\"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, I did another cross validation for this model showing the average and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85074627, 0.895     , 0.9       , 0.88      , 0.86      ,\n",
       "       0.905     , 0.905     , 0.875     , 0.88      , 0.85427136])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_gym =  cross_val_score(clf,X_gym, target_gym, cv=10)\n",
    "cross_val_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (cross_val_gym.mean(), cross_val_gym.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our last category, we can see that in average it performs quite well in comparison with the previous dataset trained clasifiers. This is bery likely because of all our datasets, this one holds a more balanced distribution of positive to negative results. Ending as shown below, with a higher precision and recal for negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[198  51]\n",
      " [ 12 339]]\n",
      "The accuracy is: 0.895\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.80      0.86       249\n",
      "    positive       0.87      0.97      0.91       351\n",
      "\n",
      "    accuracy                           0.90       600\n",
      "   macro avg       0.91      0.88      0.89       600\n",
      "weighted avg       0.90      0.90      0.89       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_hotel = df_hotel['Text']\n",
    "target_hotel = df_hotel['Score']\n",
    "\n",
    "\n",
    "X_hotel = vectorizer.fit_transform(document_hotel)\n",
    "\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "data_train, data_test, target_train, target_test = train_test_split(X_hotel, target_hotel, test_size=0.30)\n",
    "\n",
    "clf.fit(data_train,target_train)\n",
    "predict = clf.predict(data_test)\n",
    "\n",
    "cm = confusion_matrix(target_test, predict,labels=['negative','positive'])\n",
    "print(cm)\n",
    "print(\"The accuracy is:\", accuracy_score(target_test, predict))\n",
    "print(\"\\n\")\n",
    "print(classification_report(target_test, predict, target_names=[\"negative\",\"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have again a cross validiation for hotel reviews dataset. As we can see the average accuracy is in the same range as the previous two dataset classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90049751, 0.87064677, 0.85572139, 0.90547264, 0.88      ,\n",
       "       0.84      , 0.85427136, 0.88442211, 0.82914573, 0.8241206 ])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_hotel =  cross_val_score(clf,X_hotel, target_hotel, cv=10)\n",
    "cross_val_hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (cross_val_hotel.mean(), cross_val_hotel.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought it would be interesting to join all the datasets into one and train/test our classifier using all the reviews. Below are the statistics, and as we can see it performed very well in all parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[208  41]\n",
      " [ 13 338]]\n",
      "The accuracy is: 0.8988888888888888\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.84      0.86       659\n",
      "    positive       0.91      0.94      0.92      1141\n",
      "\n",
      "    accuracy                           0.90      1800\n",
      "   macro avg       0.89      0.89      0.89      1800\n",
      "weighted avg       0.90      0.90      0.90      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_all = df_all['Text']\n",
    "target_all = df_all['Score']\n",
    "\n",
    "\n",
    "X_all = vectorizer.fit_transform(document_all)\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "data_train, data_test, target_train, target_test = train_test_split(X_all, target_all, test_size=0.30)\n",
    "\n",
    "clf.fit(data_train,target_train)\n",
    "predict = clf.predict(data_test)\n",
    "\n",
    "print(cm)\n",
    "print(\"The accuracy is:\", accuracy_score(target_test, predict))\n",
    "cm = confusion_matrix(target_test, predict,labels=['negative','positive'])\n",
    "print(\"\\n\")\n",
    "print(classification_report(target_test, predict, target_names=[\"negative\",\"positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross validation below shows the average (one of the highest) of accuracy for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8968386 , 0.85524126, 0.86189684, 0.88333333, 0.87333333,\n",
       "       0.88      , 0.89333333, 0.90651085, 0.84974958, 0.88146912])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_all =  cross_val_score(clf,X_all, target_all, cv=10)\n",
    "cross_val_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (cross_val_all.mean(), cross_val_all.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also wanted to show the number of positive and negative reviews in our three datasets.\n",
    "\n",
    "As we can see, for all of the datasets there are more positive reviews than negatives, with hotel being the most balanced. This bias in our data could make it difficult to train our classifier to detect negative reviews, and as shown above this is precisely what happened. For all of our datasets, the recal for our negative reviews were muck lower than the positives.\n",
    "\n",
    "Another factor that I believed affected our outcome was the manner in which we divided star based ratings into positive and negative. If we read the reviews for three stars, we could debate wether the review was a positive one or negative. This makes it so many of the previously rated 3-stars are classified as positive. I would consider making 3-stars into positive or a third class \"neutral\". The issue that might raise from doing this is having even less negative reviews in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>positive</td>\n",
       "      <td>got sent dealership doing repair work mercedes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1212</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Score                                               Text\n",
       "count       2000                                               2000\n",
       "unique         2                                               2000\n",
       "top     positive  got sent dealership doing repair work mercedes...\n",
       "freq        1212                                                  1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_auto.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>positive</td>\n",
       "      <td>son wanted try jujitsu location highly recomme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1299</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Score                                               Text\n",
       "count       2000                                               2000\n",
       "unique         2                                               2000\n",
       "top     positive  son wanted try jujitsu location highly recomme...\n",
       "freq        1299                                                  1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gym.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>positive</td>\n",
       "      <td>location sweet away minutes busy las vegas str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1176</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Score                                               Text\n",
       "count       2000                                               2000\n",
       "unique         2                                               2000\n",
       "top     positive  location sweet away minutes busy las vegas str...\n",
       "freq        1176                                                  1"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hotel.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we are going to train our classifier using one entire dataset, and test it on the other two. I expect it to show a similar but slightly lower accuracy, as certain terms and words that only apply for a specific category.\n",
    "\n",
    "### Auto dataset trained classifier on Gym and Hotel data\n",
    "\n",
    "We can see below that in average the auto dataset trained classifier performed well in predicting positive and negative reviews at around 85% accuracy. We can also observe similar patterns seen before of a lower recall on negative than positve and around the same precision on both. This is mainly due to the disparity in total negative against positive reviews in our data and the fact that many \"negative\" reviews hold positive terms (3-star reviews). As a result her and throughout all of the evaluations, negative recall has been significantly lower than positive, as of all the actual negative reviews a lower portion were classified as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of using Auto model on Gym data is: 0.8555\n",
      "\n",
      "\n",
      "[[ 512  189]\n",
      " [ 100 1199]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.73      0.78       701\n",
      "    positive       0.86      0.92      0.89      1299\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.85      0.83      0.84      2000\n",
      "weighted avg       0.85      0.86      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create variables with all texts and scores from our datasets.\n",
    "document_auto = df_auto['Text']\n",
    "target_auto = df_auto['Score']\n",
    "document_gym = df_gym['Text']\n",
    "target_gym = df_gym['Score']\n",
    "document_hotel = df_hotel['Text']\n",
    "target_hotel = df_hotel['Score']\n",
    "\n",
    "#create vectors and store separately for all of our text\n",
    "X_auto = vectorizer.fit_transform(document_auto)\n",
    "X_gym = vectorizer.transform(document_gym)\n",
    "X_hotel = vectorizer.transform(document_hotel)\n",
    "\n",
    "#set the train data as the vector for auto\n",
    "data_train = X_auto\n",
    "#set the train target as the auto target\n",
    "target_train =  target_auto\n",
    "#use gym as our test data and target\n",
    "data_test = X_gym\n",
    "target_test =  target_gym\n",
    "#run the classifier on our training data\n",
    "clf.fit(data_train,target_train)\n",
    "#run model on our test data\n",
    "predict = clf.predict(data_test)\n",
    "\n",
    "#print accuracy, confusion matrix, and classification report\n",
    "print(\"The accuracy of using Auto model on Gym data is:\", accuracy_score(target_test, predict))\n",
    "print(\"\\n\")\n",
    "cm = confusion_matrix(target_test, predict,labels=['negative','positive'])\n",
    "print(cm)\n",
    "print(\"\\n\")\n",
    "print(classification_report(target_test, predict, target_names=[\"negative\",\"positive\"]))\n",
    "#repeat for hotel data as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of using Auto model on Hotel data is: 0.8535\n",
      "\n",
      "\n",
      "[[ 659  165]\n",
      " [ 128 1048]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.80      0.82       824\n",
      "    positive       0.86      0.89      0.88      1176\n",
      "\n",
      "    accuracy                           0.85      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.85      0.85      0.85      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_test = X_hotel\n",
    "target_test =  target_hotel\n",
    "predict = clf.predict(data_test)\n",
    "print(\"The accuracy of using Auto model on Hotel data is:\", accuracy_score(target_test, predict))\n",
    "print(\"\\n\")\n",
    "cm = confusion_matrix(target_test, predict,labels=['negative','positive'])\n",
    "print(cm)\n",
    "print(\"\\n\")\n",
    "print(classification_report(target_test, predict, target_names=[\"negative\",\"positive\"]))\n",
    "#Repeat switching around our train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym dataset trained classifier on Auto and Hotel data\n",
    "\n",
    "Next, we can see that the accuracy of our Gym trained model is slightly lower when predicting reviews. Following a similar pattern with precision and recall. However, we can see that negative reviews statistics underperformed by a significant amount. This might be due to the gym dataset being the one with the fewer amount of negative reviews. Which means even less negative scenarios covered in the training process. This had an obvious result on all negative review statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of using Gym model on Auto data is: 0.803\n",
      "\n",
      "\n",
      "[[625 163]\n",
      " [231 981]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.79      0.76       788\n",
      "    positive       0.86      0.81      0.83      1212\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.79      0.80      0.80      2000\n",
      "weighted avg       0.81      0.80      0.80      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_auto = vectorizer.fit_transform(document_auto)\n",
    "X_gym = vectorizer.transform(document_gym)\n",
    "X_hotel = vectorizer.transform(document_hotel)\n",
    "\n",
    "data_train = X_gym\n",
    "target_train =  target_gym\n",
    "data_test = X_auto\n",
    "target_test =  target_auto\n",
    "clf.fit(data_train,target_train)\n",
    "predict = clf.predict(data_test)\n",
    "\n",
    "\n",
    "print(\"The accuracy of using Gym model on Auto data is:\", accuracy_score(target_test, predict))\n",
    "print(\"\\n\")\n",
    "cm = confusion_matrix(target_test, predict,labels=['negative','positive'])\n",
    "print(cm)\n",
    "print(\"\\n\")\n",
    "print(classification_report(target_test, predict, target_names=[\"negative\",\"positive\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of using Gym model on Hotel data is: 0.829\n",
      "\n",
      "\n",
      "[[ 642  182]\n",
      " [ 160 1016]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.78      0.79       824\n",
      "    positive       0.85      0.86      0.86      1176\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.82      0.82      0.82      2000\n",
      "weighted avg       0.83      0.83      0.83      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_test = X_hotel\n",
    "target_test =  target_hotel\n",
    "predict = clf.predict(data_test)\n",
    "print(\"The accuracy of using Gym model on Hotel data is:\", accuracy_score(target_test, predict))\n",
    "print(\"\\n\")\n",
    "cm = confusion_matrix(target_test, predict,labels=['negative','positive'])\n",
    "print(cm)\n",
    "print(\"\\n\")\n",
    "print(classification_report(target_test, predict, target_names=[\"negative\",\"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotel dataset trained classifier on Auto and Gym data\n",
    "\n",
    "Lastly, we see the performance of the hotel trained classifier on the other two dataset. It follows a similar pattern to the Auto trained model with a pretty high accuracy, precision and recall. This might be because both the hotel dataset and the auto dataset have a more balanced number of positivie reviews to negative reviews, which means the classifier is better trained to predict negative reviews, while doing a great job in predicting positive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of using Hotel model on Auto data is: 0.8635\n",
      "\n",
      "\n",
      "[[ 685  103]\n",
      " [ 170 1042]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.87      0.83       788\n",
      "    positive       0.91      0.86      0.88      1212\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.86      0.86      0.86      2000\n",
      "weighted avg       0.87      0.86      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_auto = vectorizer.fit_transform(document_auto)\n",
    "X_gym = vectorizer.transform(document_gym)\n",
    "X_hotel = vectorizer.transform(document_hotel)\n",
    "\n",
    "data_train = X_hotel\n",
    "target_train =  target_hotel\n",
    "data_test = X_auto\n",
    "target_test =  target_auto\n",
    "clf.fit(data_train,target_train)\n",
    "predict = clf.predict(data_test)\n",
    "\n",
    "\n",
    "print(\"The accuracy of using Hotel model on Auto data is:\", accuracy_score(target_test, predict))\n",
    "print(\"\\n\")\n",
    "cm = confusion_matrix(target_test, predict,labels=['negative','positive'])\n",
    "print(cm)\n",
    "print(\"\\n\")\n",
    "print(classification_report(target_test, predict, target_names=[\"negative\",\"positive\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of using Hotel model on Gym data is: 0.867\n",
      "[[ 537  164]\n",
      " [ 102 1197]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.77      0.80       701\n",
      "    positive       0.88      0.92      0.90      1299\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.86      0.84      0.85      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_test = X_gym\n",
    "target_test =  target_gym\n",
    "predict = clf.predict(data_test)\n",
    "print(\"The accuracy of using Hotel model on Gym data is:\", accuracy_score(target_test, predict))\n",
    "cm = confusion_matrix(target_test, predict,labels=['negative','positive'])\n",
    "print(cm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "print(classification_report(target_test, predict, target_names=[\"negative\",\"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we can see that the model trained with the gym dataset perform poorly in comparison to the other models in particular when predicting negative reviews. It might be interesting to train our data with two dataset and test on the last one. I believe this will better train our model on negative reviews and give an over-all better classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In conclusion I chose to use this style of evaluation because it represents with rations and numerical values how well our classifier performed. While we could have used accuracy alone, using recall and precision calculations give further depth to our results and show how over-all our models did better when predicting positive reviews (due to having many more positive reviews in our data). From the evaluation done above, I beleve that my classification model performed well and predicted accurately in all the scenarios. \n",
    "\n",
    "A few changes I would sugest for future analysis would be to separate the ratings into more classes as a 3-star could be taken a positive or negative. And perhaps work with more data as we have seen the amount of negative reviews was minimal compared to our positive reviews, which as shown, affected the performance of our classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
